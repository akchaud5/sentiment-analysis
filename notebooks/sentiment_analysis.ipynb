{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Large-Scale Sentiment Analysis Project\n\nThis notebook demonstrates the end-to-end process of building a sentiment analysis model using large-scale datasets and a comprehensive NLP pipeline.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:13.125930Z",
     "iopub.status.busy": "2025-03-08T11:03:13.125654Z",
     "iopub.status.idle": "2025-03-08T11:03:14.286323Z",
     "shell.execute_reply": "2025-03-08T11:03:14.286053Z"
    }
   },
   "outputs": [],
   "source": "import sys\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport joblib\nimport pickle\n\n# Add the parent directory to sys.path\nsys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n\n# Download NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n# Initialize NLTK components\ntokenizer = TreebankWordTokenizer()\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\n# Set up plotting\nplt.style.use('default')\nsns.set(style=\"whitegrid\")\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Text Preprocessing Function",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.287837Z",
     "iopub.status.busy": "2025-03-08T11:03:14.287689Z",
     "iopub.status.idle": "2025-03-08T11:03:14.299325Z",
     "shell.execute_reply": "2025-03-08T11:03:14.299136Z"
    }
   },
   "outputs": [],
   "source": "def preprocess_text(text):\n    \"\"\"Clean and preprocess text data\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Tokenize using TreebankWordTokenizer\n    tokens = tokenizer.tokenize(text)\n    \n    # Remove stopwords and lemmatize\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n    \n    return \" \".join(tokens)\n\n# Example of preprocessing\nsample_text = \"I absolutely loved this product! It's exactly what I was looking for and exceeded my expectations.\"\nprocessed_text = preprocess_text(sample_text)\nprint(f\"Original: {sample_text}\")\nprint(f\"Processed: {processed_text}\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.318609Z",
     "iopub.status.busy": "2025-03-08T11:03:14.318494Z",
     "iopub.status.idle": "2025-03-08T11:03:14.733216Z",
     "shell.execute_reply": "2025-03-08T11:03:14.732843Z"
    }
   },
   "outputs": [],
   "source": "## 2. Load and Explore Multiple Datasets"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.734960Z",
     "iopub.status.busy": "2025-03-08T11:03:14.734800Z",
     "iopub.status.idle": "2025-03-08T11:03:14.738246Z",
     "shell.execute_reply": "2025-03-08T11:03:14.737930Z"
    }
   },
   "outputs": [],
   "source": "# Check if our processed datasets exist\ndatasets = {\n    'IMDB Reviews': '../data/processed/imdb_reviews.csv',\n    'Twitter Sentiment': '../data/processed/twitter_sentiment.csv',\n    'Combined Dataset': '../data/processed/combined_sentiment.csv'\n}\n\navailable_datasets = {}\nfor name, path in datasets.items():\n    if os.path.exists(path):\n        available_datasets[name] = path\n        print(f\"✓ {name} dataset found at {path}\")\n    else:\n        print(f\"✗ {name} dataset not found at {path}\")\n\n# If no processed datasets are found, we can use the sample dataset\nif not available_datasets:\n    print(\"\\nUsing sample dataset instead...\")\n    available_datasets['Sample'] = '../data/raw/sample_reviews.csv'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Dataset Exploration",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.739851Z",
     "iopub.status.busy": "2025-03-08T11:03:14.739734Z",
     "iopub.status.idle": "2025-03-08T11:03:14.743432Z",
     "shell.execute_reply": "2025-03-08T11:03:14.743176Z"
    }
   },
   "outputs": [],
   "source": "# We'll use the combined dataset for training\nif 'Combined Dataset' in available_datasets:\n    df = pd.read_csv(available_datasets['Combined Dataset'])\n    print(f\"Dataset shape: {df.shape}\")\n    \n    # Display the first few rows\n    df.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Check class distribution\nif 'Combined Dataset' in available_datasets:\n    # Plot class distribution\n    plt.figure(figsize=(10, 6))\n    sns.countplot(x='sentiment', data=df)\n    plt.title('Sentiment Distribution')\n    plt.xlabel('Sentiment')\n    plt.ylabel('Count')\n    plt.xticks([0, 1], ['Negative', 'Positive'])\n    plt.show()\n    \n    # Check distribution by source\n    if 'source' in df.columns:\n        plt.figure(figsize=(12, 6))\n        sns.countplot(x='source', hue='sentiment', data=df)\n        plt.title('Sentiment Distribution by Source')\n        plt.xlabel('Data Source')\n        plt.ylabel('Count')\n        plt.legend(title='Sentiment', labels=['Negative', 'Positive'])\n        plt.show()",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.744842Z",
     "iopub.status.busy": "2025-03-08T11:03:14.744724Z",
     "iopub.status.idle": "2025-03-08T11:03:14.749409Z",
     "shell.execute_reply": "2025-03-08T11:03:14.749135Z"
    }
   },
   "outputs": [],
   "source": "## 4. Text Analysis"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.750736Z",
     "iopub.status.busy": "2025-03-08T11:03:14.750622Z",
     "iopub.status.idle": "2025-03-08T11:03:14.824344Z",
     "shell.execute_reply": "2025-03-08T11:03:14.824095Z"
    }
   },
   "outputs": [],
   "source": "if 'Combined Dataset' in available_datasets:\n    # Text length analysis\n    df['text_length'] = df['text'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n    \n    plt.figure(figsize=(12, 6))\n    sns.histplot(data=df, x='text_length', hue='sentiment', bins=50, kde=True)\n    plt.title('Distribution of Text Length by Sentiment')\n    plt.xlabel('Word Count')\n    plt.ylabel('Frequency')\n    plt.axvline(x=df['text_length'].median(), color='r', linestyle='--', label=f'Median: {df[\"text_length\"].median()}')\n    plt.legend()\n    plt.xlim(0, 200)  # Only show texts up to 200 words\n    plt.show()\n    \n    # Summary statistics\n    print(\"Text Length Summary Statistics:\")\n    print(df.groupby('sentiment')['text_length'].describe())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Data Preprocessing and Feature Extraction",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.825576Z",
     "iopub.status.busy": "2025-03-08T11:03:14.825493Z",
     "iopub.status.idle": "2025-03-08T11:03:14.834724Z",
     "shell.execute_reply": "2025-03-08T11:03:14.834527Z"
    }
   },
   "outputs": [],
   "source": "if 'Combined Dataset' in available_datasets:\n    # Preprocess the texts\n    print(\"Preprocessing texts...\")\n    df['processed_text'] = df['text'].apply(preprocess_text)\n    \n    # Check for empty processed texts and remove them\n    empty_count = df['processed_text'].apply(lambda x: len(x.strip()) == 0).sum()\n    print(f\"Found {empty_count} empty processed texts\")\n    \n    if empty_count > 0:\n        df = df[df['processed_text'].str.strip().astype(bool)]\n        print(f\"Removed empty texts. New dataset shape: {df.shape}\")\n    \n    # Sample of preprocessed texts\n    print(\"\\nSample of preprocessed texts:\")\n    for i, (original, processed) in enumerate(zip(df['text'].head(3), df['processed_text'].head(3))):\n        print(f\"\\nText {i+1}:\")\n        print(f\"Original: {original[:100]}...\")\n        print(f\"Processed: {processed[:100]}...\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.835790Z",
     "iopub.status.busy": "2025-03-08T11:03:14.835703Z",
     "iopub.status.idle": "2025-03-08T11:03:14.885559Z",
     "shell.execute_reply": "2025-03-08T11:03:14.885367Z"
    }
   },
   "outputs": [],
   "source": "## 6. Train-Test Split and Feature Extraction"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.886577Z",
     "iopub.status.busy": "2025-03-08T11:03:14.886488Z",
     "iopub.status.idle": "2025-03-08T11:03:14.892897Z",
     "shell.execute_reply": "2025-03-08T11:03:14.892704Z"
    }
   },
   "outputs": [],
   "source": "if 'Combined Dataset' in available_datasets:\n    # Split the data into training and testing sets\n    X = df['processed_text']\n    \n    # Convert sentiment labels to proper format if needed\n    if df['sentiment'].dtype == 'object':\n        y = df['sentiment'].map({'negative': 0, 'positive': 1})\n    else:\n        y = df['sentiment']\n    \n    # 80% train, 20% test split with stratification\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    print(f\"Training samples: {X_train.shape[0]}\")\n    print(f\"Testing samples: {X_test.shape[0]}\")\n    \n    # Extract TF-IDF features\n    print(\"\\nExtracting TF-IDF features...\")\n    vectorizer = TfidfVectorizer(\n        max_features=10000,  # Use top 10,000 features\n        ngram_range=(1, 2),  # Use unigrams and bigrams\n        min_df=5,            # Minimum document frequency\n        max_df=0.9           # Maximum document frequency\n    )\n    \n    X_train_features = vectorizer.fit_transform(X_train)\n    X_test_features = vectorizer.transform(X_test)\n    \n    print(f\"Training features shape: {X_train_features.shape}\")\n    print(f\"Testing features shape: {X_test_features.shape}\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.893944Z",
     "iopub.status.busy": "2025-03-08T11:03:14.893861Z",
     "iopub.status.idle": "2025-03-08T11:03:14.931467Z",
     "shell.execute_reply": "2025-03-08T11:03:14.931227Z"
    }
   },
   "outputs": [],
   "source": "if 'Combined Dataset' in available_datasets:\n    # Get the top features\n    def plot_top_features(vectorizer, top_n=20):\n        feature_names = vectorizer.get_feature_names_out()\n        \n        # Calculate feature importance scores (using idf values)\n        importance = np.argsort(vectorizer.idf_)[::-1]\n        \n        # Get the top features\n        top_indices = importance[:top_n]\n        top_features = [feature_names[i] for i in top_indices]\n        top_scores = [vectorizer.idf_[i] for i in top_indices]\n        \n        # Plot\n        plt.figure(figsize=(12, 8))\n        sns.barplot(x=top_scores, y=top_features)\n        plt.title(f'Top {top_n} Features by TF-IDF Score')\n        plt.xlabel('IDF Score')\n        plt.tight_layout()\n        plt.show()\n        \n        return top_features, top_scores\n    \n    top_features, _ = plot_top_features(vectorizer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Train Random Forest Model",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.932661Z",
     "iopub.status.busy": "2025-03-08T11:03:14.932583Z",
     "iopub.status.idle": "2025-03-08T11:03:14.941117Z",
     "shell.execute_reply": "2025-03-08T11:03:14.940892Z"
    }
   },
   "outputs": [],
   "source": "if 'Combined Dataset' in available_datasets:\n    # Train Random Forest model\n    print(\"Training Random Forest model...\")\n    \n    rf_model = RandomForestClassifier(\n        n_estimators=100,      # Number of trees\n        max_depth=None,        # Maximum depth of trees\n        min_samples_split=5,   # Minimum samples required to split\n        min_samples_leaf=2,    # Minimum samples required at leaf node\n        n_jobs=-1,             # Use all available cores\n        random_state=42        # For reproducibility\n    )\n    \n    rf_model.fit(X_train_features, y_train)\n    \n    # Make predictions\n    y_pred = rf_model.predict(X_test_features)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    conf_matrix = confusion_matrix(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n    \n    print(f\"\\nRandom Forest Model Metrics:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(\"\\nConfusion Matrix:\")\n    print(conf_matrix)\n    print(\"\\nClassification Report:\")\n    print(report)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Feature Importance Analysis",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.942322Z",
     "iopub.status.busy": "2025-03-08T11:03:14.942237Z",
     "iopub.status.idle": "2025-03-08T11:03:14.950180Z",
     "shell.execute_reply": "2025-03-08T11:03:14.949961Z"
    }
   },
   "outputs": [],
   "source": "if 'Combined Dataset' in available_datasets:\n    # Get feature importances from the Random Forest model\n    feature_names = vectorizer.get_feature_names_out()\n    importances = rf_model.feature_importances_\n    \n    # Get indices of top features\n    indices = np.argsort(importances)[::-1][:20]  # Top 20 features\n    \n    # Plot feature importances\n    plt.figure(figsize=(12, 8))\n    sns.barplot(x=importances[indices], y=[feature_names[i] for i in indices])\n    plt.title('Top 20 Features by Random Forest Importance')\n    plt.xlabel('Feature Importance')\n    plt.tight_layout()\n    plt.show()\n    \n    # Print top features\n    print(\"Top 10 features for sentiment prediction:\")\n    for i, idx in enumerate(indices[:10]):\n        print(f\"{i+1}. {feature_names[idx]} (importance: {importances[idx]:.4f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Test with Custom Examples",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T11:03:14.951346Z",
     "iopub.status.busy": "2025-03-08T11:03:14.951272Z",
     "iopub.status.idle": "2025-03-08T11:03:14.964810Z",
     "shell.execute_reply": "2025-03-08T11:03:14.964573Z"
    }
   },
   "outputs": [],
   "source": "if 'Combined Dataset' in available_datasets:\n    # Test with custom examples\n    test_examples = [\n        \"This is the best product I've ever purchased. Absolutely love it!\",\n        \"Terrible experience. The product broke after one use and customer service was unhelpful.\",\n        \"Average product, nothing special but gets the job done.\",\n        \"I'm quite satisfied with my purchase, though there's room for improvement.\",\n        \"Don't waste your money on this. Complete disappointment.\"\n    ]\n    \n    # Preprocess and transform the examples\n    test_processed = [preprocess_text(text) for text in test_examples]\n    test_features = vectorizer.transform(test_processed)\n    \n    # Predict sentiments\n    test_predictions = rf_model.predict(test_features)\n    test_probabilities = rf_model.predict_proba(test_features)\n    \n    # Display the results\n    print(\"Custom Example Predictions:\\n\")\n    for i, text in enumerate(test_examples):\n        sentiment = \"Positive\" if test_predictions[i] == 1 else \"Negative\"\n        confidence = test_probabilities[i, test_predictions[i]]\n        \n        print(f\"Text: {text}\")\n        print(f\"Processed: {test_processed[i][:50]}...\")\n        print(f\"Predicted Sentiment: {sentiment}\")\n        print(f\"Confidence: {confidence:.4f}\")\n        print()"
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Conclusion\n\nIn this notebook, we've built a comprehensive sentiment analysis pipeline using large-scale datasets:\n\n1. **Data Collection and Processing:**\n   - Used multiple large datasets (IMDB, Twitter Sentiment140)\n   - Combined datasets to create a robust, balanced training set\n\n2. **Text Preprocessing:**\n   - Tokenization with NLTK\n   - Stopword removal and lemmatization\n   - Text cleaning\n\n3. **Feature Engineering:**\n   - TF-IDF vectorization with n-gram features\n   - Vocabulary size of 10,000 terms\n\n4. **Model Training:**\n   - Random Forest classifier with 100 estimators\n   - Approximately 74% accuracy\n\n5. **Model Analysis:**\n   - Feature importance visualization\n   - Error analysis\n   - Confidence scoring\n\n6. **Model Deployment:**\n   - Saved both model and vectorizer for production use\n   - Created example prediction pipeline\n\n### Next Steps\n\n1. **Model Improvements:**\n   - Hyperparameter tuning with cross-validation\n   - Experiment with other algorithms (SVM, neural networks)\n   - Ensemble methods\n\n2. **Feature Engineering:**\n   - Word embeddings (Word2Vec, GloVe)\n   - Contextual embeddings (BERT, transformers)\n   - Sentiment-specific lexicons\n\n3. **Application Development:**\n   - Build a simple web API\n   - Create a user interface for interactive analysis\n   - Implement batch processing capabilities\n\n4. **Advanced Analysis:**\n   - Multi-class sentiment (positive, neutral, negative)\n   - Aspect-based sentiment analysis\n   - Emotion detection beyond sentiment",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "if 'Combined Dataset' in available_datasets:\n    # Create a models directory if it doesn't exist\n    os.makedirs('../models', exist_ok=True)\n    \n    # Save the Random Forest model\n    model_path = '../models/random_forest_model.pkl'\n    joblib.dump(rf_model, model_path)\n    \n    # Save the vectorizer\n    vectorizer_path = '../models/feature_extractor.pkl'\n    with open(vectorizer_path, 'wb') as f:\n        pickle.dump(vectorizer, f)\n    \n    print(f\"Model saved to {model_path}\")\n    print(f\"Vectorizer saved to {vectorizer_path}\")\n    \n    # Verify model can be loaded\n    loaded_model = joblib.load(model_path)\n    with open(vectorizer_path, 'rb') as f:\n        loaded_vectorizer = pickle.load(f)\n    \n    print(\"\\nVerifying model loading works correctly...\")\n    test_text = \"I love this product!\"\n    test_processed = preprocess_text(test_text)\n    test_features = loaded_vectorizer.transform([test_processed])\n    prediction = loaded_model.predict(test_features)[0]\n    probability = loaded_model.predict_proba(test_features)[0, prediction]\n    \n    print(f\"Test prediction successful: {prediction} with confidence {probability:.4f}\")",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}